{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Week 2 lab: Keras tutorial.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pcalvachi/Metaanalysiscages/blob/master/Week_2_lab_Keras_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZlKfUT1M3yHv"
      },
      "source": [
        "\n",
        "#Lecture 2 Lab: Introduction to Keras\n",
        "\n",
        "In this example, we will use Keras to develop a multi-layer model\n",
        "\n",
        "Examples from: BMI707 Lecture 2 Lab (Spring 2018), https://scikit-learn.org/stable/documentation.html, and https://keras.io/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GwJeQhf5C6S"
      },
      "source": [
        "## Introduction to Keras\n",
        "\n",
        "To simulate a dataset and build multi-layer neural network classifiers using Keras"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U1LI2wQN3xVa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c25baf4c-3005-4427-b82e-046a79f28078"
      },
      "source": [
        "# generate simulated data as in Lecture 1 Lab\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "n_train = 2000\n",
        "n_val = 500\n",
        "n_classes = 2\n",
        "X, y = make_classification(n_samples=n_train+n_val, \n",
        "                           n_features=10,\n",
        "                           n_classes=n_classes,\n",
        "                           n_informative=2, \n",
        "                           n_redundant=0,\n",
        "                           random_state=1, \n",
        "                           shuffle=True)\n",
        "\n",
        "X_train = X[0:n_train,:]\n",
        "y_train = y[0:n_train]\n",
        "X_val = X[n_train:(n_train+n_val),:]\n",
        "y_val = y[n_train:(n_train+n_val)]\n",
        "\n",
        "print(X_train.shape)\n",
        "print(y_train.shape)\n",
        "print(X_val.shape)\n",
        "print(y_val.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2000, 10)\n",
            "(2000,)\n",
            "(500, 10)\n",
            "(500,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWQPYyzKQ157"
      },
      "source": [
        "## Question 1\n",
        "Build a sequential model in Keras. Please fill in the code below.\n",
        "\n",
        "Hint: https://keras.io/getting-started/sequential-model-guide/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnYjzqHd5ZNf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bfa4f3aa-4a74-43bc-a226-26f520a49a35"
      },
      "source": [
        "import keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
        "from tensorflow.keras.optimizers import SGD\n",
        "\n",
        "# Your code here - build an MLP with one hidden layer\n",
        "############################################################\n",
        "\n",
        "num_features = X.shape[1]\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=32, activation='relu', input_dim=num_features, name='hidden_layer'))\n",
        "model.add(Dense(units=1, activation='sigmoid', name='output_layer')) \n",
        "\n",
        "opt = SGD(learning_rate=0.1)\n",
        "\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer (Dense)         (None, 32)                352       \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 385\n",
            "Trainable params: 385\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aMchl7ihhBl",
        "outputId": "ef2f2c3d-f382-4a09-87eb-bb5e25211c29"
      },
      "source": [
        "model.fit(x=X_train, \n",
        "         y=y_train,\n",
        "         batch_size=32,\n",
        "         epochs=10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "63/63 [==============================] - 1s 1ms/step - loss: 0.4996 - accuracy: 0.7623\n",
            "Epoch 2/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.2860 - accuracy: 0.8989\n",
            "Epoch 3/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.2446 - accuracy: 0.9075\n",
            "Epoch 4/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.2512 - accuracy: 0.9077\n",
            "Epoch 5/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.2500 - accuracy: 0.9079\n",
            "Epoch 6/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.2424 - accuracy: 0.9074\n",
            "Epoch 7/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.2665 - accuracy: 0.9003\n",
            "Epoch 8/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.2435 - accuracy: 0.9103\n",
            "Epoch 9/10\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2588 - accuracy: 0.8953\n",
            "Epoch 10/10\n",
            "63/63 [==============================] - 0s 1ms/step - loss: 0.2664 - accuracy: 0.8981\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f08d83ec810>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuPtaAYc7Q8z"
      },
      "source": [
        "## Question 2\n",
        "How would you add more layers and dropout to the model above?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VRIWY6r0ROSD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b948987-41ed-4faa-d0be-4544b38f042c"
      },
      "source": [
        "# Your code here\n",
        "############################################################\n",
        "# just model.add additional layers, for example:\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(units=32, activation='selu', input_dim=num_features, name='hidden_layer'))\n",
        "model.add(Dropout(rate=0.2))\n",
        "model.add(Dense(units=32, activation='selu', name='hidden_layer_2'))\n",
        "model.add(Dense(units=1, activation='sigmoid', name='output_layer')) \n",
        "\n",
        "opt = Adam(learning_rate=0.01)\n",
        "\n",
        "model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.summary()\n",
        "\n",
        "model.fit(x=X_train, \n",
        "         y=y_train,\n",
        "         batch_size=32,\n",
        "         validation_data = (X_val, y_val),\n",
        "         epochs=50)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "hidden_layer (Dense)         (None, 32)                352       \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 32)                0         \n",
            "_________________________________________________________________\n",
            "hidden_layer_2 (Dense)       (None, 32)                1056      \n",
            "_________________________________________________________________\n",
            "output_layer (Dense)         (None, 1)                 33        \n",
            "=================================================================\n",
            "Total params: 1,441\n",
            "Trainable params: 1,441\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/50\n",
            "63/63 [==============================] - 1s 5ms/step - loss: 0.3896 - accuracy: 0.8317 - val_loss: 0.2688 - val_accuracy: 0.8960\n",
            "Epoch 2/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2570 - accuracy: 0.8898 - val_loss: 0.2902 - val_accuracy: 0.8800\n",
            "Epoch 3/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2529 - accuracy: 0.9020 - val_loss: 0.2793 - val_accuracy: 0.8920\n",
            "Epoch 4/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2235 - accuracy: 0.9110 - val_loss: 0.2788 - val_accuracy: 0.8920\n",
            "Epoch 5/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2540 - accuracy: 0.8956 - val_loss: 0.2802 - val_accuracy: 0.9020\n",
            "Epoch 6/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2352 - accuracy: 0.9073 - val_loss: 0.2691 - val_accuracy: 0.8980\n",
            "Epoch 7/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2438 - accuracy: 0.8993 - val_loss: 0.2729 - val_accuracy: 0.8920\n",
            "Epoch 8/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2498 - accuracy: 0.9019 - val_loss: 0.2782 - val_accuracy: 0.8900\n",
            "Epoch 9/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2457 - accuracy: 0.9085 - val_loss: 0.2969 - val_accuracy: 0.8840\n",
            "Epoch 10/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2571 - accuracy: 0.8975 - val_loss: 0.2778 - val_accuracy: 0.9000\n",
            "Epoch 11/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2588 - accuracy: 0.9033 - val_loss: 0.2802 - val_accuracy: 0.8880\n",
            "Epoch 12/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2572 - accuracy: 0.9061 - val_loss: 0.2853 - val_accuracy: 0.8880\n",
            "Epoch 13/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2436 - accuracy: 0.9091 - val_loss: 0.2751 - val_accuracy: 0.8940\n",
            "Epoch 14/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2504 - accuracy: 0.9054 - val_loss: 0.2894 - val_accuracy: 0.9020\n",
            "Epoch 15/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2565 - accuracy: 0.8973 - val_loss: 0.2822 - val_accuracy: 0.9000\n",
            "Epoch 16/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2635 - accuracy: 0.9007 - val_loss: 0.2958 - val_accuracy: 0.8880\n",
            "Epoch 17/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2765 - accuracy: 0.8926 - val_loss: 0.2916 - val_accuracy: 0.8960\n",
            "Epoch 18/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2311 - accuracy: 0.9200 - val_loss: 0.2866 - val_accuracy: 0.8960\n",
            "Epoch 19/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2381 - accuracy: 0.9174 - val_loss: 0.2926 - val_accuracy: 0.8900\n",
            "Epoch 20/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2469 - accuracy: 0.9045 - val_loss: 0.2808 - val_accuracy: 0.8920\n",
            "Epoch 21/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2406 - accuracy: 0.9124 - val_loss: 0.2874 - val_accuracy: 0.8840\n",
            "Epoch 22/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2478 - accuracy: 0.9059 - val_loss: 0.2701 - val_accuracy: 0.8940\n",
            "Epoch 23/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2358 - accuracy: 0.9181 - val_loss: 0.2411 - val_accuracy: 0.9080\n",
            "Epoch 24/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2182 - accuracy: 0.9242 - val_loss: 0.2593 - val_accuracy: 0.8980\n",
            "Epoch 25/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2469 - accuracy: 0.9083 - val_loss: 0.2471 - val_accuracy: 0.9020\n",
            "Epoch 26/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2252 - accuracy: 0.9153 - val_loss: 0.2406 - val_accuracy: 0.9100\n",
            "Epoch 27/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2096 - accuracy: 0.9195 - val_loss: 0.2509 - val_accuracy: 0.9060\n",
            "Epoch 28/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2167 - accuracy: 0.9317 - val_loss: 0.2288 - val_accuracy: 0.9060\n",
            "Epoch 29/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2099 - accuracy: 0.9160 - val_loss: 0.2201 - val_accuracy: 0.9100\n",
            "Epoch 30/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1998 - accuracy: 0.9237 - val_loss: 0.2165 - val_accuracy: 0.9160\n",
            "Epoch 31/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2052 - accuracy: 0.9322 - val_loss: 0.2325 - val_accuracy: 0.9160\n",
            "Epoch 32/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2044 - accuracy: 0.9266 - val_loss: 0.1945 - val_accuracy: 0.9280\n",
            "Epoch 33/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.1830 - accuracy: 0.9391 - val_loss: 0.2212 - val_accuracy: 0.9120\n",
            "Epoch 34/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2113 - accuracy: 0.9248 - val_loss: 0.2169 - val_accuracy: 0.9180\n",
            "Epoch 35/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2102 - accuracy: 0.9233 - val_loss: 0.1945 - val_accuracy: 0.9240\n",
            "Epoch 36/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.1753 - accuracy: 0.9380 - val_loss: 0.2419 - val_accuracy: 0.9120\n",
            "Epoch 37/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.2371 - accuracy: 0.9285 - val_loss: 0.1810 - val_accuracy: 0.9220\n",
            "Epoch 38/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1798 - accuracy: 0.9327 - val_loss: 0.2088 - val_accuracy: 0.9240\n",
            "Epoch 39/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1513 - accuracy: 0.9475 - val_loss: 0.1939 - val_accuracy: 0.9220\n",
            "Epoch 40/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1453 - accuracy: 0.9475 - val_loss: 0.1930 - val_accuracy: 0.9220\n",
            "Epoch 41/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1599 - accuracy: 0.9401 - val_loss: 0.1738 - val_accuracy: 0.9320\n",
            "Epoch 42/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1777 - accuracy: 0.9342 - val_loss: 0.2104 - val_accuracy: 0.9140\n",
            "Epoch 43/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.1546 - accuracy: 0.9486 - val_loss: 0.1799 - val_accuracy: 0.9300\n",
            "Epoch 44/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1739 - accuracy: 0.9368 - val_loss: 0.1870 - val_accuracy: 0.9300\n",
            "Epoch 45/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.2028 - accuracy: 0.9331 - val_loss: 0.1924 - val_accuracy: 0.9300\n",
            "Epoch 46/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1850 - accuracy: 0.9354 - val_loss: 0.1688 - val_accuracy: 0.9400\n",
            "Epoch 47/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1691 - accuracy: 0.9460 - val_loss: 0.1913 - val_accuracy: 0.9280\n",
            "Epoch 48/50\n",
            "63/63 [==============================] - 0s 2ms/step - loss: 0.1846 - accuracy: 0.9396 - val_loss: 0.1718 - val_accuracy: 0.9320\n",
            "Epoch 49/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1811 - accuracy: 0.9352 - val_loss: 0.1744 - val_accuracy: 0.9260\n",
            "Epoch 50/50\n",
            "63/63 [==============================] - 0s 3ms/step - loss: 0.1611 - accuracy: 0.9406 - val_loss: 0.1990 - val_accuracy: 0.9160\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f08d1e35410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "piz07x2DhhvC"
      },
      "source": [
        "## Question 3\n",
        "What's the validation loss and accuracy?\n",
        "\n",
        "Hint: https://keras.io/models/model/#evaluate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0kotVW96u-m",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc39f43-32a5-49f3-cf63-04af16a74bff"
      },
      "source": [
        "# Your code here\n",
        "############################################################\n",
        "score = model.evaluate(X_val, y_val, batch_size=128)\n",
        "print(score)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4/4 [==============================] - 0s 3ms/step - loss: 0.1990 - accuracy: 0.9160\n",
            "[0.19900478422641754, 0.9160000085830688]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}